{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import toad\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ProcessPoolExecutor, wait, ALL_COMPLETED\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc, roc_curve\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.aggr.attention import AttentionalAggregation\n",
    "from torch_geometric.nn import Linear, HeteroConv, GraphConv, GAT, RGCNConv, BatchNorm, GCN\n",
    "from torch_geometric.nn.models import JumpingKnowledge\n",
    "\n",
    "from ema import EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "# logger.add(sys.stdout, format=\"[{time:YYYY-MM-DD :mm:ss}] {level} {message}\", colorize=True)\n",
    "# logger.add(\"./train.log\", format=\"[{time:YYYY-MM-DD :mm:ss}] {level} {message}\", rotation=\"5MB\", encoding=\"utf-8\", enqueue=True, retention=\"5 days\", colorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def load_pickle(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def save_pickle(obj, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_graphs(dataset, edge_lag=1, weight_base=1.05, cache=True):\n",
    "    if cache and os.path.exists(f\"cache/{os.path.basename(dataset)}.pkl\"):\n",
    "        return load_pickle(f\"cache/{os.path.basename(dataset)}.pkl\")\n",
    "\n",
    "    pickle_files = glob(dataset + \"/*.pkl\")\n",
    "\n",
    "    idx = []\n",
    "    graphs = []\n",
    "    label_dict = {\"00\": 0, \"10\": 1}\n",
    "\n",
    "    source = []\n",
    "    target = []\n",
    "    for i in range(1, edge_lag + 1):\n",
    "        source.extend([i for i in range(256 - i)])\n",
    "        target.extend([i for i in range(i, 256)])\n",
    "\n",
    "    for file in tqdm(pickle_files, desc=f\"load {dataset} data :::\"):\n",
    "        idx.append(os.path.basename(file))\n",
    "\n",
    "        features, metadata = load_pickle(file)\n",
    "        \n",
    "        mileage = metadata.get(\"mileage\")\n",
    "        label = label_dict[metadata.get(\"label\", \"00\")]\n",
    "\n",
    "        features = pd.DataFrame(features, columns=['volt','current','soc','max_single_volt','min_single_volt','max_temp','min_temp','timestamp'])\n",
    "        features[\"mileage\"] = mileage\n",
    "        features[\"timestamp\"] = (features[\"timestamp\"] - features[\"timestamp\"].min()).astype(int)\n",
    "        # features[f\"timestamp_diff_{i}\"] = features[\"timestamp\"].diff().fillna(-1) # .apply(lambda x: math.log(x+1, 60))\n",
    "\n",
    "        features[\"resistance\"] = features[\"volt\"] / (-features[\"current\"])\n",
    "        features[\"pwoer\"] = features[\"volt\"] * (-features[\"current\"])\n",
    "        features[\"consistency_volt\"] = features[\"volt\"].std()\n",
    "        features[\"consistency_soc\"] = features[\"soc\"].std()\n",
    "        features[\"consistency_temp\"] = (features.max_temp - features.min_temp).std()\n",
    "        features[\"consistency_resistance\"] = (features[\"volt\"] / (-features[\"current\"])).std()\n",
    "        features[\"single_volt_range\"] = features.max_single_volt - features.min_single_volt\n",
    "        features[\"temp_range\"] = features.max_temp - features.min_temp\n",
    "        \n",
    "        for i in [5, 15, 30, 60]:\n",
    "            features[f\"timestamp_diff_{i}\"] = features[\"timestamp\"].diff(periods=i).fillna(-1)\n",
    "            features[f\"volt_div_{i}\"] = features[\"volt\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "            features[f\"current_div_{i}\"] = features[\"current\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "            features[f\"soc_div_{i}\"] = features[\"soc\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "            features[f\"max_single_volt_div_{i}\"] = features[\"max_single_volt\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "            features[f\"min_single_volt_div_{i}\"] = features[\"min_single_volt\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "            features[f\"max_temp_div_{i}\"] = features[\"max_temp\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "            features[f\"min_temp_div_{i}\"] = features[\"min_temp\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "\n",
    "            features[f\"volt_change_{i}\"] = features[\"volt\"].pct_change(periods=i).fillna(0)\n",
    "            features[f\"current_change_{i}\"] = features[\"current\"].pct_change(periods=i).fillna(0)\n",
    "            features[f\"soc_change_{i}\"] = features[\"soc\"].pct_change(periods=i).fillna(0)\n",
    "            features[f\"max_single_volt_change_{i}\"] = features[\"max_single_volt\"].pct_change(periods=i).fillna(0)\n",
    "            features[f\"min_single_volt_change_{i}\"] = features[\"min_single_volt\"].pct_change(periods=i).fillna(0)\n",
    "            features[f\"max_temp_change_{i}\"] = features[\"max_temp\"].pct_change(periods=i).fillna(0)\n",
    "            features[f\"min_temp_change_{i}\"] = features[\"min_temp\"].pct_change(periods=i).fillna(0)\n",
    "\n",
    "            features[f\"volt_lag_{i}\"] = features[\"volt\"].shift(i).fillna(features[\"volt\"][0])\n",
    "            features[f\"current_lag_{i}\"] = features[\"current\"].shift(i).fillna(features[\"current\"][0])\n",
    "            features[f\"soc_lag_{i}\"] = features[\"soc\"].shift(i).fillna(features[\"soc\"][0])\n",
    "            features[f\"max_single_volt_lag_{i}\"] = features[\"max_single_volt\"].shift(i).fillna(features[\"max_single_volt\"][0])\n",
    "            features[f\"min_single_volt_lag_{i}\"] = features[\"min_single_volt\"].shift(i).fillna(features[\"min_single_volt\"][0])\n",
    "            features[f\"max_temp_lag_{i}\"] = features[\"max_temp\"].shift(i).fillna(features[\"max_temp\"][0])\n",
    "            features[f\"min_temp_lag_{i}\"] = features[\"min_temp\"].shift(i).fillna(features[\"min_temp\"][0])\n",
    "            features[f\"single_volt_range_lag_{i}\"] = features[\"single_volt_range\"].shift(i).fillna(features[\"single_volt_range\"][0])\n",
    "            features[f\"temp_range_lag_{i}\"] = features[\"temp_range\"].shift(i).fillna(features[\"temp_range\"][0])\n",
    "\n",
    "            # if i > 1:\n",
    "            #     for col in ['volt','current','soc','max_single_volt','min_single_volt','max_temp','min_temp', 'single_volt_range', 'temp_range']:\n",
    "            #         features[f'{col}_rolling_mean_{i}'] = features[col].rolling(window=i, center=True).mean().fillna(features[col].mean())\n",
    "            #         features[f'{col}_rolling_max_{i}'] = features[col].rolling(window=i, center=True).max().fillna(features[col].max())\n",
    "            #         features[f'{col}_rolling_min_{i}'] = features[col].rolling(window=i, center=True).min().fillna(features[col].min())\n",
    "            #         features[f'{col}_rolling_std_{i}'] = features[col].rolling(window=i, center=True).std().fillna(features[col].std())\n",
    "            #         features[f'{col}_rolling_median_{i}'] = features[col].rolling(window=i, center=True).median().fillna(features[col].median())\n",
    "            #         if i > 3:\n",
    "            #             features[f'{col}_rolling_skew_{i}'] = features[col].rolling(window=i, center=True).skew().fillna(features[col].skew())\n",
    "            #             features[f'{col}_rolling_kurt_{i}'] = features[col].rolling(window=i, center=True).kurt().fillna(features[col].kurt())\n",
    "\n",
    "        x = torch.FloatTensor(features.drop(columns=[col for col in [\"file_name\", \"label\"] if col in features.columns]).values.tolist())\n",
    "        edge_index = torch.LongTensor([source, target])\n",
    "        edge_weight = np.power(weight_base, (features[\"timestamp\"].loc[source].values - features[\"timestamp\"].loc[target].values + 1))\n",
    "        edge_attr = torch.FloatTensor(edge_weight)\n",
    "\n",
    "        graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=torch.LongTensor([label]))\n",
    "\n",
    "        graphs.append(graph)\n",
    "\n",
    "    save_pickle((graphs, idx), f\"cache/{os.path.basename(dataset)}.pkl\")\n",
    "\n",
    "    return graphs, idx\n",
    "\n",
    "\n",
    "def separate_data(graphs, fold_idx, seed=3407, flod=5):\n",
    "    assert 0 <= fold_idx and fold_idx < flod, \"fold_idx must be from 0 to 9.\"\n",
    "\n",
    "    labels = [graph.y.cpu().numpy().tolist()[0] for graph in graphs]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=flod, shuffle=True, random_state=seed)\n",
    "    \n",
    "    idx_list = []\n",
    "    for idx in skf.split(np.zeros(len(labels)), labels):\n",
    "        idx_list.append(idx)\n",
    "    \n",
    "    train_idx, test_idx = idx_list[fold_idx]\n",
    "    train_graphs = [graphs[i] for i in train_idx]\n",
    "    test_graphs = [graphs[i] for i in test_idx]\n",
    "\n",
    "    return train_graphs, test_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file, source, target, weight_base=1.05):\n",
    "    label_dict = {\"00\": 0, \"10\": 1}\n",
    "    features, metadata = load_pickle(file)\n",
    "    \n",
    "    mileage = metadata.get(\"mileage\")\n",
    "    label = label_dict[metadata.get(\"label\", \"00\")]\n",
    "\n",
    "    features = pd.DataFrame(features, columns=['volt','current','soc','max_single_volt','min_single_volt','max_temp','min_temp','timestamp'])\n",
    "    features[\"mileage\"] = mileage\n",
    "    features[\"timestamp\"] = (features[\"timestamp\"] - features[\"timestamp\"].min()).astype(int)\n",
    "    # features[f\"timestamp_diff_{i}\"] = features[\"timestamp\"].diff().fillna(-1) # .apply(lambda x: math.log(x+1, 60))\n",
    "\n",
    "    features[\"resistance\"] = features[\"volt\"] / (-features[\"current\"])\n",
    "    features[\"pwoer\"] = features[\"volt\"] * (-features[\"current\"])\n",
    "    features[\"consistency_volt\"] = features[\"volt\"].std()\n",
    "    features[\"consistency_soc\"] = features[\"soc\"].std()\n",
    "    features[\"consistency_temp\"] = (features.max_temp - features.min_temp).std()\n",
    "    features[\"consistency_resistance\"] = (features[\"volt\"] / (-features[\"current\"])).std()\n",
    "    features[\"single_volt_range\"] = features.max_single_volt - features.min_single_volt\n",
    "    features[\"temp_range\"] = features.max_temp - features.min_temp\n",
    "    \n",
    "    for i in [1, 5, 15, 30, 60]:\n",
    "        features[f\"timestamp_diff_{i}\"] = features[\"timestamp\"].diff(periods=i).fillna(-1)\n",
    "        features[f\"volt_div_{i}\"] = features[\"volt\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "        features[f\"current_div_{i}\"] = features[\"current\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "        features[f\"soc_div_{i}\"] = features[\"soc\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "        features[f\"max_single_volt_div_{i}\"] = features[\"max_single_volt\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "        features[f\"min_single_volt_div_{i}\"] = features[\"min_single_volt\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "        features[f\"max_temp_div_{i}\"] = features[\"max_temp\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "        features[f\"min_temp_div_{i}\"] = features[\"min_temp\"].diff(i).fillna(0) / features[f\"timestamp_diff_{i}\"]\n",
    "\n",
    "        features[f\"volt_change_{i}\"] = features[\"volt\"].pct_change(periods=i).fillna(0)\n",
    "        features[f\"current_change_{i}\"] = features[\"current\"].pct_change(periods=i).fillna(0)\n",
    "        features[f\"soc_change_{i}\"] = features[\"soc\"].pct_change(periods=i).fillna(0)\n",
    "        features[f\"max_single_volt_change_{i}\"] = features[\"max_single_volt\"].pct_change(periods=i).fillna(0)\n",
    "        features[f\"min_single_volt_change_{i}\"] = features[\"min_single_volt\"].pct_change(periods=i).fillna(0)\n",
    "        features[f\"max_temp_change_{i}\"] = features[\"max_temp\"].pct_change(periods=i).fillna(0)\n",
    "        features[f\"min_temp_change_{i}\"] = features[\"min_temp\"].pct_change(periods=i).fillna(0)\n",
    "\n",
    "        features[f\"volt_lag_{i}\"] = features[\"volt\"].shift(i).fillna(features[\"volt\"][0])\n",
    "        features[f\"current_lag_{i}\"] = features[\"current\"].shift(i).fillna(features[\"current\"][0])\n",
    "        features[f\"soc_lag_{i}\"] = features[\"soc\"].shift(i).fillna(features[\"soc\"][0])\n",
    "        features[f\"max_single_volt_lag_{i}\"] = features[\"max_single_volt\"].shift(i).fillna(features[\"max_single_volt\"][0])\n",
    "        features[f\"min_single_volt_lag_{i}\"] = features[\"min_single_volt\"].shift(i).fillna(features[\"min_single_volt\"][0])\n",
    "        features[f\"max_temp_lag_{i}\"] = features[\"max_temp\"].shift(i).fillna(features[\"max_temp\"][0])\n",
    "        features[f\"min_temp_lag_{i}\"] = features[\"min_temp\"].shift(i).fillna(features[\"min_temp\"][0])\n",
    "        features[f\"single_volt_range_lag_{i}\"] = features[\"single_volt_range\"].shift(i).fillna(features[\"single_volt_range\"][0])\n",
    "        features[f\"temp_range_lag_{i}\"] = features[\"temp_range\"].shift(i).fillna(features[\"temp_range\"][0])\n",
    "\n",
    "        # if i > 1:\n",
    "        #     for col in ['volt','current','soc','max_single_volt','min_single_volt','max_temp','min_temp', 'single_volt_range', 'temp_range']:\n",
    "        #         features[f'{col}_rolling_mean_{i}'] = features[col].rolling(window=i, center=True).mean().fillna(features[col].mean())\n",
    "        #         features[f'{col}_rolling_max_{i}'] = features[col].rolling(window=i, center=True).max().fillna(features[col].max())\n",
    "        #         features[f'{col}_rolling_min_{i}'] = features[col].rolling(window=i, center=True).min().fillna(features[col].min())\n",
    "        #         features[f'{col}_rolling_std_{i}'] = features[col].rolling(window=i, center=True).std().fillna(features[col].std())\n",
    "        #         features[f'{col}_rolling_median_{i}'] = features[col].rolling(window=i, center=True).median().fillna(features[col].median())\n",
    "        #         if i > 3:\n",
    "        #             features[f'{col}_rolling_skew_{i}'] = features[col].rolling(window=i, center=True).skew().fillna(features[col].skew())\n",
    "        #             features[f'{col}_rolling_kurt_{i}'] = features[col].rolling(window=i, center=True).kurt().fillna(features[col].kurt())\n",
    "        \n",
    "    features = features.replace(np.inf, -1).fillna(0)\n",
    "\n",
    "    x = torch.FloatTensor(features.drop(columns=[col for col in [\"file_name\", \"label\"] if col in features.columns]).values.tolist())\n",
    "    edge_index = torch.LongTensor([source, target])\n",
    "    edge_weight = np.power(weight_base, (-np.abs(features[\"timestamp\"].loc[source].values - features[\"timestamp\"].loc[target].values) + 1))\n",
    "    edge_attr = torch.FloatTensor(edge_weight)\n",
    "\n",
    "    graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=torch.LongTensor([label]))\n",
    "\n",
    "    return graph, os.path.basename(file)\n",
    "\n",
    "\n",
    "def load_graphs(dataset, edge_lag=1, weight_base=1.05, cache=True, pool=None, no_direction=False):\n",
    "    if cache and os.path.exists(f\"cache/{os.path.basename(dataset)}.pkl\"):\n",
    "        return load_pickle(f\"cache/{os.path.basename(dataset)}.pkl\")\n",
    "\n",
    "    pickle_files = glob(dataset + \"/*.pkl\")\n",
    "\n",
    "    idx = []\n",
    "    graphs = []\n",
    "    label_dict = {\"00\": 0, \"10\": 1}\n",
    "\n",
    "    source = []\n",
    "    target = []\n",
    "    for i in range(1, edge_lag + 1):\n",
    "        source.extend([i for i in range(256 - i)])\n",
    "        target.extend([i for i in range(i, 256)])\n",
    "\n",
    "        if no_direction:\n",
    "            source.extend([i for i in range(i, 256)])\n",
    "            target.extend([i for i in range(256-i)])\n",
    "\n",
    "    if pool:\n",
    "        tasks = []\n",
    "        for file in pickle_files:\n",
    "            tasks.append(pool.submit(process, file, source, target))\n",
    "\n",
    "        wait(tasks, return_when=ALL_COMPLETED)\n",
    "\n",
    "        for task in tasks:\n",
    "            if task.result() is not None:\n",
    "                graph, file = task.result()\n",
    "                graphs.append(graph)\n",
    "                idx.append(file)\n",
    "\n",
    "    else:\n",
    "        for file in tqdm(pickle_files, desc=f\"load {dataset} data :::\"):\n",
    "            idx.append(os.path.basename(file))\n",
    "            \n",
    "            graph, _ = process(file, source, target)\n",
    "            graphs.append(graph)\n",
    "\n",
    "    save_pickle((graphs, idx), f\"cache/{os.path.basename(dataset)}.pkl\")\n",
    "\n",
    "    return graphs, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(\"UGformer\", formatter_class=argparse.ArgumentDefaultsHelpFormatter, conflict_handler='resolve')\n",
    "# parser.add_argument(\"--train\", default=\"../data/Train\", help=\"\")\n",
    "# parser.add_argument(\"--test\", default=\"../data/Test_A\", help=\"Name of the dataset.\")\n",
    "# parser.add_argument(\"--learning_rate\", default=0.001, type=float, help=\"Learning rate\")\n",
    "# parser.add_argument(\"--num_epochs\", default=50, type=int, help=\"Number of training epochs\")\n",
    "# parser.add_argument(\"--model_name\", default='PTC', help=\"\")\n",
    "# parser.add_argument(\"--dropout\", default=0.5, type=float, help=\"\")\n",
    "# parser.add_argument(\"--num_hidden_layers\", default=1, type=int, help=\"\")\n",
    "# parser.add_argument(\"--nhead\", default=1, type=int, help=\"\")\n",
    "# parser.add_argument(\"--num_timesteps\", default=1, type=int, help=\"Number of self-attention layers within each UGformer layer\")\n",
    "# parser.add_argument(\"--ff_hidden_size\", default=256, type=int, help=\"The hidden size for the feedforward layer\")\n",
    "# parser.add_argument('--fold_idx', type=int, default=1, help='The fold index. 0-9.')\n",
    "# parser.add_argument(\"--seed\", type=int, default=3407, help=\"The random seed.\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"data/Train\"\n",
    "test = \"data/Test_A\"\n",
    "batch_size = 2048\n",
    "learning_rate = 3e-4\n",
    "num_epochs =25\n",
    "dropout = 0.5\n",
    "seed = 3407\n",
    "num_flods = 5\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data... finished!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# with ProcessPoolExecutor(max_workers=64) as pool:\n",
    "pool = None\n",
    "_train_graphs, _train_index = load_graphs(train, cache=True, pool=pool, no_direction=True)\n",
    "# _train_graphs = []\n",
    "# for i in range(1, 4):\n",
    "#     _train_graphs.extend(load_pickle(f\"cache/{os.path.basename(train)}_{i}.pkl\"))\n",
    "_test_graphs, test_index = load_graphs(test, cache=True, pool=pool, no_direction=True)\n",
    "\n",
    "num_features = _train_graphs[0].x.shape[1]\n",
    "\n",
    "print(\"Loading data... finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = _train_graphs[0].features\n",
    "cols_index= [cols.index(col) for col in ['volt', 'current', 'soc', 'max_single_volt', 'min_single_volt', 'max_temp', 'min_temp', 'timestamp', 'mileage', 'timestamp_diff', 'pwoer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = [Data(x=graph.x[:, cols_index], edge_index=graph.edge_index, edge_attr=graph.edge_attr, y=graph.y) for graph in _train_graphs]\n",
    "test_graphs = [Data(x=graph.x[:, cols_index], edge_index=graph.edge_index, edge_attr=graph.edge_attr, y=graph.y) for graph in _test_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(_test_graphs[0].graph_attr, _test_graphs[0].x[-1, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_features(graph):\n",
    "    # return graph.graph_attr.numpy()\n",
    "    # return np.append(graph.graph_attr.numpy(), graph.x[-1, :])\n",
    "    loader_train = DataLoader(GraphaDataset([graph]), batch_size=1, shuffle=False)\n",
    "    for b in loader_train:\n",
    "        return torch.cat([model(b.to(device), embedding=True).cpu()[0], b.graph_attr.cpu().detach()]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_features = []\n",
    "\n",
    "def cv_model(clf, clf_name, cv=5, seed=2022):\n",
    "\n",
    "    test_x = np.array([lgb_features(graph) for graph in _test_graphs])\n",
    "    _mean = np.mean(test_x, axis=0)\n",
    "    _std = np.std(test_x, axis=0)\n",
    "    test_x = (test_x - _mean) / (_std + 1e-4)\n",
    "\n",
    "    test = np.zeros(len(test_x))\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    for i in range(num_flods):\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trin_graphs, vail_graphs = separate_data(_train_graphs, i, seed=seed, flod=num_flods)\n",
    "        trn_x, trn_y = np.array([lgb_features(graph) for graph in trin_graphs]), np.array([graph.y.numpy()[0] for graph in trin_graphs])\n",
    "        val_x, val_y = np.array([lgb_features(graph) for graph in vail_graphs]), np.array([graph.y.numpy()[0] for graph in vail_graphs])\n",
    "\n",
    "        _mean = np.mean(trn_x, axis=0)\n",
    "        _std = np.std(trn_x, axis=0)\n",
    "        trn_x = (trn_x - _mean) / (_std + 1e-4)\n",
    "        val_x = (val_x - _mean) / (_std + 1e-4)\n",
    "\n",
    "        features = [f\"x_{i}\" for i in range(trin_graphs[0].x.shape[-1])]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 4,\n",
    "                'num_leaves': 2 ** 6,\n",
    "                'lambda_l2': 10,\n",
    "                'lambda_l1': 0.2,\n",
    "                'feature_fraction': 0.7,\n",
    "                'bagging_fraction': 0.7,\n",
    "                'bagging_freq': 10,\n",
    "                'learning_rate': 0.001,\n",
    "                'max_bin': 8,\n",
    "                'min_data_in_leaf': 256,\n",
    "                'seed': 3048,\n",
    "                'n_jobs':-1,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "\n",
    "            model = clf.train(params, train_matrix, num_boost_round=2500, valid_sets=[train_matrix, valid_matrix], \n",
    "                              categorical_feature=[], verbose_eval=100, early_stopping_rounds=200)\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "\n",
    "            importance_features.extend([i[0] for i in list(sorted(zip(features, model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20]])\n",
    "            \n",
    "            print(list(sorted(zip(features, model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20])\n",
    "            \n",
    "        if clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            \n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'binary:logistic',\n",
    "                      'eval_metric': 'auc',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 4,\n",
    "                      'max_bin': 10,\n",
    "                      'gamma': 1,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.0001,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2020,\n",
    "                      'nthread': -1,\n",
    "                      'verbosity': 1,\n",
    "                      }\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(params, train_matrix, num_boost_round=1500, evals=watchlist, verbose_eval=100, early_stopping_rounds=200)\n",
    "            val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred = model.predict(test_matrix , ntree_limit=model.best_ntree_limit)\n",
    "            \n",
    "        test = test_pred / num_flods\n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "        print(cv_scores)\n",
    "    \n",
    "    print(\"%s_scotrainre_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model():\n",
    "    lgb_test = cv_model(lgb, \"lgb\")\n",
    "    return lgb_test\n",
    "\n",
    "def xgb_model():\n",
    "    xgb_test = cv_model(xgb, \"xgb\")\n",
    "    return xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgb_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "#区间缩放，返回值为缩放到[0, 1]区间的数据\n",
    "# Standard_data=MinMaxScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# cols = ['volt','current','soc','max_single_volt','min_single_volt','max_temp', 'min_temp', 'resistance', 'pwoer', 'single_volt_range', 'temp_range']\n",
    "# funcs = [\"sum\", \"min\", \"max\", \"std\", \"median\", \"skew\", \"kurt\", \"mad\"]\n",
    "# columns = [\"mileage\"] + [{c: f} for f, c in itertools.product(funcs, cols)]\n",
    "# print([columns[int(x[2:])] for x in sorted(set(importance_features))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(test_index, pred)), columns=[\"file_name\", \"score\"]).to_csv(\"lgb_submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"lgb_submit.csv\")['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"results_cv1/flod_5_epoch_11submit.csv\")[\"score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = 0.3 * MinMaxScaler().fit_transform(pd.read_csv(\"lgb_submit.csv\")[['score']])[:, 0] + 0.7 * MinMaxScaler().fit_transform(pd.read_csv(\"results_cv1/flod_5_epoch_11submit.csv\")[['score']])[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphaDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super().__init__()\n",
    "        self.data, self.slices = self.collate(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateset_train = GraphaDataset(train_graphs)\n",
    "dateset_test = GraphaDataset(test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(dateset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dateset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    for batch in loader_train:\n",
    "        print(summary(model, input_data=(batch)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.conv1 = GCNConv(dateset_train.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn1 = BatchNorm(hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dateset_train.num_classes)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        # 1. 获得节点嵌入\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # 3. 分类器\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNTest(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden)\n",
    "        self.bn1 = BatchNorm(hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden, hidden))\n",
    "        # self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.bn1.reset_parameters()\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        # self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data, embedding=False):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        if self.training:\n",
    "            x = self.add_noise(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        if embedding:\n",
    "            return x\n",
    "\n",
    "        # x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_noise(x, perturb_noise=0.025):\n",
    "        perturb = torch.empty_like(x).uniform_(-perturb_noise, perturb_noise)\n",
    "        return x + perturb\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNWithJK(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, mode='cat'):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden)\n",
    "        self.bn1 = BatchNorm(hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden, hidden))\n",
    "        self.jump = JumpingKnowledge(mode)\n",
    "        if mode == 'cat':\n",
    "            self.lin1 = Linear(num_layers * hidden, hidden)\n",
    "        else:\n",
    "            self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.bn1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.jump.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        xs = [x]\n",
    "        x = self.bn1(x)\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            xs += [x]\n",
    "        x = self.jump(xs)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import JumpingKnowledge, SAGEConv, global_mean_pool\n",
    "\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_features, hidden)\n",
    "        self.bn1 = BatchNorm(hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden, hidden))\n",
    "        # self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.bn1.reset_parameters()\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if self.training:\n",
    "            x = self.add_noise(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        if self.training:\n",
    "            x = self.add_noise(x)\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_noise(x, perturb_noise=0.05):\n",
    "        perturb = torch.empty_like(x).uniform_(-perturb_noise, perturb_noise)\n",
    "        return x + perturb\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = GCNNet(hidden_channels=128).to(device)\n",
    "model = GCNTest(dateset_train, 4, 256).to(device)\n",
    "# model = GCNTest(dateset_train, 4, 128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.02)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "ema = EMA(model.parameters(), decay=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_total = len(dateset_trin) // batch_size * num_epochs\n",
    "# # scheduler = get_default_cosine_schedule_with_warmup(optimizer, t_total, warmup_ratio=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1, 2, 3], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "\n",
    "    for data in tqdm(loader, desc=\"training :::\"):\n",
    "        data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        # ema.update(model.parameters())\n",
    "\n",
    "        # data_iter.set_postfix(loss='{:.4f}'.format(loss.cpu().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, mode=\"train\"):\n",
    "    model.eval()\n",
    "    preps = []\n",
    "    trues = []\n",
    "\n",
    "    # ema.store(model.parameters())\n",
    "    # ema.copy_to(model.parameters())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc=f\"evaluate :::\"):\n",
    "            data.to(device)\n",
    "            trues.extend(data.y.cpu().numpy().tolist())\n",
    "            out = model(data)\n",
    "            pred = F.softmax(out)[:, 1]\n",
    "            preps.extend(pred.cpu().numpy().tolist())\n",
    "    \n",
    "    # ema.restore(model.parameters())\n",
    "\n",
    "    return roc_auc_score(trues, preps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(loader, submit=\"results/submit.csv\", download=True, use_ema=False):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    if use_ema:\n",
    "        ema.store(model.parameters())\n",
    "        ema.copy_to(model.parameters())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader_test, desc=\"inference :::\"):\n",
    "            data.to(device)\n",
    "            out = model(data)\n",
    "            results.extend(F.softmax(out)[:, 1].cpu().numpy().tolist())\n",
    "\n",
    "    if use_ema:\n",
    "        ema.restore(model.parameters())\n",
    "\n",
    "    if submit:\n",
    "        pd.DataFrame(list(zip(test_index, results)), columns=[\"file_name\", \"score\"]).to_csv(submit, index=False)\n",
    "\n",
    "        if download:\n",
    "            print(f\"sshpass -p '' scp vloong/{submit} results/\")\n",
    "\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:08<00:00,  1.62it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 000, train auc: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.11it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 001, train auc: 0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.12it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 002, train auc: 0.8761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.13it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 003, train auc: 0.8807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.15it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 004, train auc: 0.8828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.18it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 005, train auc: 0.8855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.14it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 006, train auc: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.17it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 007, train auc: 0.8914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.18it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 008, train auc: 0.8972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:05<00:00,  2.43it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:03<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 009, train auc: 0.8978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:05<00:00,  2.44it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:03<00:00,  4.51it/s]\n",
      "inference :::: 100%|██████████| 6234/6234 [00:23<00:00, 269.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 010, train auc: 0.9096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:06<00:00,  2.10it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:02<00:00,  5.94it/s]\n",
      "inference :::: 100%|██████████| 6234/6234 [00:22<00:00, 274.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 011, train auc: 0.9181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:03<00:00,  4.18it/s]\n",
      "inference :::: 100%|██████████| 6234/6234 [00:22<00:00, 271.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 012, train auc: 0.9228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training :::: 100%|██████████| 14/14 [00:05<00:00,  2.36it/s]\n",
      "evaluate :::: 100%|██████████| 14/14 [00:03<00:00,  4.15it/s]\n",
      "inference :::: 100%|██████████| 6234/6234 [00:22<00:00, 274.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 013, train auc: 0.9248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(14):\n",
    "    train(loader_train)\n",
    "    train_auc = test(loader_train, mode=\"train\")\n",
    "    if epoch >= 10 and epoch % 1 == 0:\n",
    "        inference(loader_test, submit=f\"results/{epoch}_result.csv\", download=False)\n",
    "        # inference(loader_test, submit=f\"results/{epoch}_ema_result.csv\", download=False, use_ema=True)\n",
    "    \n",
    "    print(f'epoch: {epoch:03d}, train auc: {train_auc:.4f}')\n",
    "    \n",
    "    count = 0\n",
    "    if count == 0 and train_auc > 0.93:\n",
    "        scheduler.step()\n",
    "        count += 1\n",
    "        \n",
    "    if train_auc > 0.935:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference :::: 100%|██████████| 6234/6234 [00:22<00:00, 274.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshpass -p 'QW3ee425#c5bd!4713=a67ddf*e04c3e34c892' scp share@113.31.111.86:/data/lpzhang/vloong/results/submit.csv /Users/lubberit/Downloads/results/\n"
     ]
    }
   ],
   "source": [
    "inference(loader_test, submit=\"results/submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_flods):\n",
    "    trin_graphs, vail_graphs = separate_data(train_graphs, i, seed=seed, flod=num_flods)\n",
    "    \n",
    "    # 创建 dataset\n",
    "    dateset_trin = GraphaDataset(trin_graphs)\n",
    "    dateset_vail = GraphaDataset(vail_graphs)\n",
    "\n",
    "    # 创建 dataloader\n",
    "    loader_trin = DataLoader(dateset_trin, batch_size=batch_size, shuffle=True)\n",
    "    loader_vail = DataLoader(dateset_vail, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 创建 model 和 optimizer\n",
    "    # model = GCNNet(hidden_channels=128).to(device)\n",
    "    model = GCNTest(dateset_train, 4, 256).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1, 2, 3], gamma=0.4)\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(f'{\"/\" * 50} flod : {i + 1}   epoch : {epoch + 1} {\"/\" * 50}')\n",
    "        \n",
    "        train(loader_trin)\n",
    "\n",
    "        train_auc = test(loader_trin, epoch)\n",
    "        test_auc = test(loader_vail, epoch)\n",
    "\n",
    "        logger.info(f'flod {i + 1}, epoch: {epoch:03d}, train auc: {train_auc:.4f}, val auc: {test_auc:.4f}')\n",
    "\n",
    "        count = 0\n",
    "        if count == 0 and test_auc > 0.925:\n",
    "            scheduler.step()\n",
    "        if test_auc > 0.935:\n",
    "            break\n",
    "    \n",
    "    if not os.path.exists(\"results_cv\"):\n",
    "        os.makedirs(\"results_cv\")\n",
    "\n",
    "    inference(loader_test, submit=f\"results_cv/flod_{i+1}_epoch_{epoch + 1}submit.csv\", download=False)\n",
    "\n",
    "    del model, optimizer, loader_trin, loader_vail, dateset_trin, dateset_vail, trin_graphs, vail_graphs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([0.8576, 0.8488, 0.8588, 0.8564, 0.8595, 0.8561])\n",
    "scores = scores / scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submit = pd.read_csv('results_cv1/submit.csv')\n",
    "final_submit[\"score\"] = final_submit[\"score\"] * scores[-1]\n",
    "for idx, f in enumerate([\n",
    "            'results_cv1/flod_1_epoch_32submit.csv',\n",
    "            'results_cv1/flod_2_epoch_13submit.csv',\n",
    "            'results_cv1/flod_3_epoch_11submit.csv',\n",
    "            'results_cv1/flod_4_epoch_10submit.csv',\n",
    "            'results_cv1/flod_5_epoch_11submit.csv',\n",
    "        ]):\n",
    "    submit = pd.read_csv(f)\n",
    "    final_submit[\"score\"] = final_submit[\"score\"] + submit[\"score\"] * scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(\"results/submit.csv\")\n",
    "final_submit[\"score\"] = final_submit[\"score\"] + submit[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submit.to_csv(\"submit_cv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最优 AUC 阈值搜索\n",
    "def Find_Optimal_Cutoff(TPR, FPR, threshold):\n",
    "    y = TPR - FPR\n",
    "    Youden_index = np.argmax(y)\n",
    "    optimal_threshold = threshold[Youden_index]\n",
    "    print(optimal_threshold)\n",
    "    point = [FPR[Youden_index], TPR[Youden_index]]\n",
    "    return optimal_threshold, point\n",
    "\n",
    "def acu_curve(index_name,y,prob):\n",
    "    font = {'family': 'Times New Roman', 'size': 12}\n",
    "    sns.set(font_scale=1.2)\n",
    "    plt.rc('font', family='Times New Roman')\n",
    "    fpr, tpr, thresholds = roc_curve(y,prob)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, threshold=thresholds)\n",
    "    print(optimal_point)\n",
    "    plt.plot(optimal_point[0], optimal_point[1], marker='o', color='r')\n",
    "    plt.text(\n",
    "                optimal_point[0], \n",
    "                optimal_point[1], \n",
    "                (\n",
    "                    float('%.2f'% optimal_point[0]),\n",
    "                    float('%.2f'% optimal_point[1])\n",
    "                ),\n",
    "                ha='right', \n",
    "                va='top', \n",
    "                fontsize=12\n",
    "            )\n",
    "    plt.text(optimal_point[0], optimal_point[1],  f'Threshold:{optimal_th:.2f}', fontsize=12)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate',fontsize = 14)\n",
    "    plt.ylabel('True Positive Rate',fontsize = 14)\n",
    "    plt.title('ROC analysis of '+ index_name,fontsize = 14)\n",
    "    plt.legend(loc=\"lower right\",fontsize = 12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_threshold(loader, model):\n",
    "    model.eval()\n",
    "    preps = []\n",
    "    trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, desc=f\"evaluate :::\"):\n",
    "            data.to(device)\n",
    "            trues.extend(data.y.cpu().numpy().tolist())\n",
    "            out = model(data)\n",
    "            pred = F.softmax(out)[:, 1]\n",
    "            preps.extend(pred.cpu().numpy().tolist())\n",
    "\n",
    "    acu_curve(\"vloong\", trues, preps)\n",
    "\n",
    "    return roc_auc_score(trues, preps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_threshold(loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
